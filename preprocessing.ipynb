{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "import string\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social isolation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/raw-datasets/socialisolation.csv')\n",
    "\n",
    "df['text'] = df[['title', 'abstract']].apply(lambda x: '. '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# bigram = Phrases()\n",
    "\n",
    "text_cleaned = []\n",
    "is_intervention_list = []\n",
    "\n",
    "# Replace all numbers with special strings\n",
    "regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "\n",
    "porter = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    text = row['text'].replace('Copyright', '').split('©', 1)[0]\n",
    "    # with stemming\n",
    "#     text = [porter.stem(word.strip()) for word in nltk.word_tokenize(text.lower()) if (word not in string.punctuation) and (word not in stopwords.words(\"english\"))]\n",
    "    \n",
    "#     # without stemming\n",
    "#     text = [word.strip() for word in nltk.word_tokenize(text.lower()) if (word not in string.punctuation) and (word not in stopwords.words(\"english\"))]\n",
    "    \n",
    "    # with lemmatizer\n",
    "    text = [wordnet_lemmatizer.lemmatize(word.strip()) for word in nltk.word_tokenize(text.lower()) if (word not in string.punctuation) and (word not in stopwords.words(\"english\"))]\n",
    "    \n",
    "#     bigram.add_vocab([text])   \n",
    "#     text_cleaned.append(text)\n",
    "#     is_intervention_list.append(row['is_intervention'])\n",
    "    text_cleaned.append(' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_with_collocation = []\n",
    "# for t in text_cleaned:\n",
    "#     text_with_collocation.append(' '.join(bigram[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_with_collocation = np.array(text_with_collocation)\n",
    "# df_cleaned = pd.DataFrame(np.c_[text_with_collocation, df['has intervention'].values, df['in_out'].values], columns=['text', 'is_intervention', 'is_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.DataFrame(np.c_[text_cleaned, df['has intervention'].values, df['in_out'].values], columns=['text', 'is_intervention', 'is_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv('datasets/cleaned-datasets/socialisolation_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proton-beam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/raw-datasets/proton-beam.csv')\n",
    "\n",
    "text_cleaned = []\n",
    "is_intervention_list = []\n",
    "\n",
    "# Replace all numbers with special strings\n",
    "regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "\n",
    "porter = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    text = row['text'].replace('Copyright', '').split('©', 1)[0]\n",
    "    # with stemming\n",
    "#     text = [porter.stem(word.strip()) for word in nltk.word_tokenize(text.lower()) if (word not in string.punctuation) and (word not in stopwords.words(\"english\"))]\n",
    "    \n",
    "#     # without stemming\n",
    "#     text = [word.strip() for word in nltk.word_tokenize(text.lower()) if (word not in string.punctuation) and (word not in stopwords.words(\"english\"))]\n",
    "    \n",
    "    # with lemmatizer\n",
    "    text = [wordnet_lemmatizer.lemmatize(word.strip()) for word in nltk.word_tokenize(text.lower()) if (word not in string.punctuation) and (word not in stopwords.words(\"english\"))]\n",
    "    \n",
    "    text_cleaned.append(' '.join(text))\n",
    "    \n",
    "\n",
    "is_IN_list = []\n",
    "for l in df['label'].values:\n",
    "    if l == -1:\n",
    "        is_IN_list.append(0)\n",
    "    else:\n",
    "        is_IN_list.append(1)\n",
    "\n",
    "df_cleaned = pd.DataFrame(list(zip(text_cleaned, is_IN_list)), columns=['text', 'is_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv('datasets/cleaned-datasets/proton-beam_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
